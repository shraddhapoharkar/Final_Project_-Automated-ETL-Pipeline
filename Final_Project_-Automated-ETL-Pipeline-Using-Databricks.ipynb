{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067783d9-97c5-4c08-8887-b5139f2bdd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3610e59d-fcc6-46b2-8e55-52c7382f2a02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4fb2f3c7-c8b2-4bc2-a7fe-0c81c8498734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.system(\"pip install -r https://raw.githubusercontent.com/George-Michael-Dagogo/World_news_tutorial/main/requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6207f39-0707-4b70-942d-5ee9d9f4f771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from newspaper import Article, Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c938c6b8-a268-4aeb-b551-8df8c126d32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da76b9fc-75e8-4df4-bce7-b3e99c0c7be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d9674d-c41d-4282-9153-619b72048cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f975cb-140a-4b51-b233-b23b291e7fcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa3bc1e-e88d-4968-ae4c-3edd2ff89120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141c8647-e7d4-4e7a-97be-36461f7cd4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to extract content from a URL\n",
    "def full_content(url):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "    config = Config()\n",
    "    config.browser_user_agent = user_agent\n",
    "    page = Article(url, config=config)\n",
    "\n",
    "    try:\n",
    "        page.download()\n",
    "        page.parse()\n",
    "        return page.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving content from {url}: {e}\")\n",
    "        return 'couldnt retrieve'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed9a8456-51e9-4146-a27a-70dca4de9782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def count_words_without_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return len(filtered_words)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores['compound'], sentiment_scores['compound']\n",
    "\n",
    "def full_content(url):\n",
    "    # Dummy implementation for the full_content function\n",
    "    return \"This is a dummy content for the URL.\"\n",
    "\n",
    "def extract_transform_function():\n",
    "    today = date.today()                        \n",
    "    yesterday = today - timedelta(days=1)     \n",
    "    day_before_yesterday = today - timedelta(days=2)   \n",
    "    \n",
    "    # Initialize the News API client with an API key\n",
    "    newsapi = NewsApiClient(api_key='ff4373852c2343a98303951439854f8c')\n",
    "\n",
    "    top_headlines = newsapi.get_top_headlines(category='entertainment',language='en',page_size=90,page=1)\n",
    "\n",
    "    articles = top_headlines.get('articles', [])  # Extract articles from the API response\n",
    "\n",
    "    # Creating DF from the articles, selecting specific columns\n",
    "    init_df = pd.DataFrame(articles, columns=['source', 'title', 'publishedAt', 'author', 'url'])\n",
    "\n",
    "    init_df['source'] = init_df['source'].apply(lambda x: x['name'] if pd.notna(x) and 'name' in x else None)\n",
    "\n",
    "    init_df['publishedAt'] = pd.to_datetime(init_df['publishedAt'])\n",
    "    filtered_df = init_df[(init_df['publishedAt'].dt.date == day_before_yesterday) | \n",
    "                          (init_df['publishedAt'].dt.date == yesterday)]\n",
    "    filtered_df.rename(columns={'publishedAt': 'date_posted'}, inplace=True)\n",
    "    \n",
    "    df = filtered_df.copy()  # Copy of filtered DF\n",
    "\n",
    "    df['content'] = df['url'].apply(full_content)  # Apply the full_content function to each URL in the DataFrame\n",
    "    \n",
    "    df['content'] = df['content'].str.replace('\\n', ' ')  # Replace newlines in the 'content' column with spaces\n",
    "    \n",
    "    df = df[df['content'] != 'couldnt retrieve']  # Filter out rows where the content could not be retrieved\n",
    "\n",
    "    # Download the NLTK stopwords dataset and other required datasets\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    # Apply the word count function to the 'content' column\n",
    "    df['word_count'] = df['content'].apply(count_words_without_stopwords)\n",
    "\n",
    "    \n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()    # Initialize the SentimentIntensityAnalyzer\n",
    "\n",
    "    # Apply the sentiment analysis function to the 'content' column\n",
    "    df[['sentiment', 'compound_score']] = df['content'].astype(str).apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "\n",
    "    \n",
    "\n",
    "    return df\n",
    "        \n",
    "dataframe = extract_transform_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3346d7a2-32bb-408c-96fa-7d51e30e8cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to count words excluding stopwords\n",
    "def count_words_without_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return len(filtered_words)\n",
    "\n",
    "# Function for sentiment analysis\n",
    "def get_sentiment(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores['compound'], sentiment_scores['compound']\n",
    "\n",
    "def full_content(url):\n",
    "    # Dummy implementation for the full_content function\n",
    "    return \"This is a dummy content for the URL.\"\n",
    "\n",
    "# Data extraction and transformation\n",
    "def extract_transform_function():\n",
    "    today = date.today()                        \n",
    "    yesterday = today - timedelta(days=1)     \n",
    "    day_before_yesterday = today - timedelta(days=2)   \n",
    "    \n",
    "    # Initialize News API client\n",
    "    newsapi = NewsApiClient(api_key='ff4373852c2343a98303951439854f8c')\n",
    "\n",
    "    top_headlines = newsapi.get_top_headlines(category='entertainment', language='en', page_size=90, page=1)\n",
    "\n",
    "    articles = top_headlines.get('articles', [])  # Extract articles\n",
    "\n",
    "    # Convert articles to DataFrame\n",
    "    init_df = pd.DataFrame(articles, columns=['source', 'title', 'publishedAt', 'author', 'url'])\n",
    "\n",
    "    # Extract source name safely\n",
    "    init_df['source'] = init_df['source'].apply(lambda x: x['name'] if pd.notna(x) and 'name' in x else None)\n",
    "\n",
    "    # Convert 'publishedAt' to date format and filter\n",
    "    init_df['publishedAt'] = pd.to_datetime(init_df['publishedAt'])\n",
    "    filtered_df = init_df[(init_df['publishedAt'].dt.date == day_before_yesterday) | \n",
    "                          (init_df['publishedAt'].dt.date == yesterday)]\n",
    "    \n",
    "    filtered_df.rename(columns={'publishedAt': 'date_posted'}, inplace=True)\n",
    "    \n",
    "    df = filtered_df.copy()  # Copy DataFrame\n",
    "    df['content'] = df['url'].apply(full_content)  # Fetch article content\n",
    "    \n",
    "    # Clean text formatting\n",
    "    df['content'] = df['content'].str.replace('\\n', ' ')\n",
    "    \n",
    "    # Remove entries where content couldn't be retrieved\n",
    "    df = df[df['content'] != 'couldnt retrieve']\n",
    "\n",
    "    # Download required NLTK datasets\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "    # Calculate word count\n",
    "    df['word_count'] = df['content'].apply(count_words_without_stopwords)\n",
    "\n",
    "\n",
    "    # Perform sentiment analysis\n",
    "    df[['sentiment', 'compound_score']] = df['content'].astype(str).apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "# Extract transformed data\n",
    "dataframe = extract_transform_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdd4dd31-af69-4934-8239-a73d09683ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d4ef9c8-81c4-4aeb-ae0b-a0d56ebaaf28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(dataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d77b36-dd26-46ab-b376-dff7d2af1e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS the_news;\n",
    "CREATE TABLE IF NOT EXISTS the_news.news_table (\n",
    "source STRING,\n",
    "title STRING,\n",
    "date_posted DATE,\n",
    "author STRING,\n",
    "url STRING,\n",
    "content STRING,\n",
    "word_count INT,\n",
    "sentiment STRING,\n",
    "compound_score DOUBLE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c205cd27-feaa-4c88-87fb-4c7a9cc79005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql DESCRIBE FORMATTED the_news.news_table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10097129-aef7-424d-ae38-ec5c359a7459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateTableExample\").getOrCreate()\n",
    "\n",
    "# Convert sentiment to string for Spark compatibility\n",
    "dataframe['sentiment'] = dataframe['sentiment'].fillna('').astype(str)\n",
    "\n",
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"date_posted\", DateType(), True), \n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"word_count\", IntegerType(), True),\n",
    "    StructField(\"sentiment\", StringType(), True),\n",
    "    StructField(\"compound_score\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(dataframe, schema=schema)\n",
    "\n",
    "# ***CHANGE HERE: Use 'append' instead of 'overwrite'***\n",
    "spark_df.write.mode('append').saveAsTable('the_news.news_table')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e62179-9911-4115-bbcc-efabe186ce7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from the_news.news_table"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5587320950950927,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Final_Project_-Automated-ETL-Pipeline-Using-Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
